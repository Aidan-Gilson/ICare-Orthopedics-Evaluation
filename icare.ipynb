{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import eutils\n",
    "import requests\n",
    "import xml\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ortho_terms(word, terms):\n",
    "    for x in terms:\n",
    "        if x.lower() in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "ortho_terms = ['spine', 'ortho', 'knee', 'foot', 'musclulo', 'bone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"medical.txt\", \"r\", encoding=\"utf8\")\n",
    "abrv_dict = {}\n",
    "for line in f.readlines():\n",
    "    line = line.strip().split('\\t')\n",
    "    abrv_dict[line[0].lower()] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_excel('ortho_providers.xlsx')\n",
    "bad_ortho = pickle.load(open('bad_ortho.pkl', 'rb'))\n",
    "# abrv_dict = pickle.load(open('abrv_dict.pkl', 'rb'))\n",
    "ortho_journals_full = pickle.load(open('ortho_journals_full.pkl', 'rb'))\n",
    "ortho_journals = set([])\n",
    "bad_count = 0\n",
    "good_count = 0\n",
    "for x in bad_ortho:\n",
    "    # print(x)\n",
    "    if x.lower() in abrv_dict:\n",
    "        for i in abrv_dict[x.lower()]:\n",
    "            ortho_journals.add(i)\n",
    "        good_count += 1\n",
    "    else:            \n",
    "        bad_count += 1\n",
    "\n",
    "for x in abrv_dict:\n",
    "    if check_ortho_terms(x, ortho_terms):\n",
    "        for i in abrv_dict[x]:\n",
    "            ortho_journals.add(i)\n",
    "\n",
    "print(bad_count)\n",
    "print(good_count)\n",
    "\n",
    "print(len(ortho_journals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add eutils API key here\n",
    "ec = eutils.Client(api_key=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esearch_query(payload, retmax = 100000, sleep=0.34):\n",
    "    \"\"\"\n",
    "    Return identifiers using the ESearch E-utility.\n",
    "    \"\"\"\n",
    "    url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    payload['rettype'] = 'xml'\n",
    "    payload['retmax'] = retmax\n",
    "    payload['retstart'] = 0\n",
    "    ids = list()\n",
    "    count = 1\n",
    "    while payload['retstart'] < count:\n",
    "        response = requests.get(url, params=payload)\n",
    "        tree = xml.etree.ElementTree.fromstring(response.text)\n",
    "        count = int(tree.findtext('Count'))\n",
    "        ids += [id_.text for id_ in tree.findall('IdList/Id')]\n",
    "        payload['retstart'] += retmax\n",
    "        # print('esearch {:.3%} complete'.format(payload['retstart'] / count), end='\\r')\n",
    "        time.sleep(sleep)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaks up a list l into sublists of size n\n",
    "def divide_chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of PMIDs and ortho journals cross references which of these PMIDs were published in orthopedic journals.\n",
    "def get_ortho_pubs(pubmed_ids, ortho_journals):\n",
    "    count = 0\n",
    "    authors = []\n",
    "    if len(pubmed_ids) > 0:\n",
    "        id_lists = list(divide_chunks(list(pubmed_ids), 250))\n",
    "        for id_chunk in id_lists:\n",
    "            paset = ec.efetch(db='pubmed', id=id_chunk)\n",
    "            for pa in paset:\n",
    "                if pa.jrnl.lower() in ortho_journals or check_ortho_terms(pa.jrnl.lower(), ortho_terms):\n",
    "                    count += 1\n",
    "                    authors.append(pa.authors)\n",
    "        if len(authors) == 0:\n",
    "            for id_chunk in id_lists:\n",
    "                paset = ec.efetch(db='pubmed', id=pubmed_ids)\n",
    "                for pa in paset:\n",
    "                    authors.append(pa.authors)\n",
    "    return count, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an author last name get PMIDs that match the authors last name in ortho journals and then check which author name formats could match the original author\n",
    "def author_check(author_last, ortho_journals):\n",
    "    payload = {'db': 'pubmed', 'term': author_last}\n",
    "    pubmed_ids = set(esearch_query(payload))\n",
    "    journal_count, authors = get_ortho_pubs(pubmed_ids, ortho_journals)\n",
    "    viable_authors = {}\n",
    "    for author_group in authors:\n",
    "        for author in author_group:\n",
    "            if author_last in author.lower():\n",
    "                if author.lower() not in viable_authors:\n",
    "                    viable_authors[author.lower()] = 1\n",
    "                else:\n",
    "                    viable_authors[author.lower()] += 1\n",
    "    return viable_authors\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the best fitting name in Pubmed for a potential orthopedic author\n",
    "def get_author_name(author):\n",
    "    viable_authors = author_check(author, ortho_journals)\n",
    "    max_key =''\n",
    "    max_value = 0\n",
    "    if len(viable_authors) > 0:\n",
    "        for key in viable_authors:\n",
    "            if viable_authors[key] > max_value:\n",
    "                max_key = key\n",
    "                max_value = viable_authors[key]\n",
    "    return max_key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs through the list of all authors under investigation and finds their best matching name.\n",
    "author_best_names = {}\n",
    "names_list = []\n",
    "bad_authors = []\n",
    "start_name = {}\n",
    "for index, x in names.iterrows():\n",
    "    name = x.Name\n",
    "    name = name.split(\",\")\n",
    "    name = name[0].split(\" \")\n",
    "    name = name[-1].lower() + ' ' + name[0][0].lower()\n",
    "    names_list.append(name)\n",
    "    start_name[x.Name.split(\", \")[0].lower()] = name\n",
    "for name in tqdm(names_list):\n",
    "    if name not in author_best_names:\n",
    "        try:\n",
    "            search_name = get_author_name(name)\n",
    "            sleep(1)\n",
    "            author_best_names[name] = search_name\n",
    "        except:\n",
    "            bad_authors.append(name)\n",
    "        pickle.dump(author_best_names,  open('author_best_names.pkl', 'wb'))\n",
    "        pickle.dump(bad_authors,  open('bad_authors.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a list of full names of Journals in the field of Orthopedics, change the format to match\n",
    "# the pubmed abbreviation. \n",
    "ortho_abrvs = set([])\n",
    "ortho_done = set([])\n",
    "bad_ortho = set([])\n",
    "\n",
    "for x in tqdm(ortho_journals_full):\n",
    "    if x not in ortho_done or x in bad_ortho:\n",
    "        if x[-5:] == ', the':\n",
    "            x = x[:-5]\n",
    "        results = ec.esearch(db='pubmed', term=f'{x}[journal]')\n",
    "        if len(results.ids) != 0:\n",
    "            paset = ec.efetch(db='pubmed', id=results.ids)\n",
    "            for pa in paset:\n",
    "                x = x + ', the'\n",
    "                ortho_abrvs.add(pa.jrnl)\n",
    "                ortho_done.add(x)\n",
    "                if x in bad_ortho:\n",
    "                    bad_ortho.remove(x)\n",
    "                break\n",
    "        else:\n",
    "            bad_ortho.add(x)\n",
    "            ortho_done.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ortho_abrvs,  open('ortho_abrvs.pkl', 'wb'))\n",
    "pickle.dump(bad_ortho,  open('bad_ortho.pkl', 'wb'))\n",
    "pickle.dump(author_best_names,  open('author_best_names.pkl', 'wb'))\n",
    "author_best_names_reverse = {author_best_names[x]:x for x in author_best_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_best_names_reverse = {author_best_names[x]:x for x in author_best_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Icare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic url request funciton to be used for ICare\n",
    "def parse2(url):\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = Options()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--log-level=3')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_added_journals = '''Eur Spine J\n",
    "\n",
    "Bone Res\n",
    "\n",
    "Clin Spine Surg\n",
    "\n",
    "J Orthop Surg\n",
    "\n",
    "J Pediatr Orthop B\n",
    "\n",
    "Bone Res\n",
    "\n",
    "J Bone Joint Surg Am\n",
    "\n",
    "J Spinal Disord Tech\n",
    "\n",
    "J Orthop Res\n",
    "\n",
    "Skeletal Radiol\n",
    "\n",
    "J Orthop Trauma\n",
    "\n",
    "Iowa Orthop J\n",
    "\n",
    "Scoliosis Spinal Disorders\n",
    "\n",
    "Int J Sports Phys Ther\n",
    "\n",
    "J Shoulder Elbow Surg\n",
    "\n",
    "Arthroscopy\n",
    "\n",
    "J Bone Joint Surg Am\n",
    "\n",
    "J Foot Ankle Surg Am\n",
    "\n",
    "J Hand Surg Am\n",
    "\n",
    "Orthop Nurs'''\n",
    "\n",
    "test = manually_added_journals.split('\\n')[::2]\n",
    "for x in manually_added_journals:\n",
    "    if x not in ortho_abrvs:\n",
    "        ortho_abrvs.add(x)\n",
    "\n",
    "manual_review_added_journals = pd.read_excel('manual_review_added_journals.xlsx')\n",
    "manual_review_added_journals = manual_review_added_journals.dropna()\n",
    "manual_review_added_journals = manual_review_added_journals[manual_review_added_journals['Include Journal (Y/N)'] == 'yes']\n",
    "manual_review_added_journals = list(manual_review_added_journals.Journals.values)\n",
    "for x in manual_review_added_journals:\n",
    "    ortho_abrvs.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all names to a standardized format\n",
    "name_dic = {}\n",
    "for index, x in names.iterrows():\n",
    "    name = x.Name\n",
    "    name = name.split(\",\")\n",
    "    name = name[0].split(\" \")\n",
    "    name = name[-1].lower() + ' ' + name[0][0].lower()\n",
    "    name_dic[x.Name.split(\", \")[0].lower()] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_check(name):\n",
    "    name = name_dic[author_best_names_reverse[name]]\n",
    "    name = name.split(\",\")\n",
    "    name = name[0].split(\" \")\n",
    "    name = name[0].lower() + ' ' + name[-1].lower()\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a query on ICare, returns which found results were in orthopedic journals.\n",
    "def find_pmids(soup, ortho_abrvs):\n",
    "    scripts = soup.find_all('script')\n",
    "    if len(scripts) == 0:\n",
    "        wanted_pmids = []\n",
    "    wanted_pmids = []\n",
    "    pmids = []\n",
    "    journal_names = []\n",
    "    for x in scripts:\n",
    "        if x.text != '':\n",
    "            chunks = x.text.split('\\n')[1].strip()[19:-1].split(', ')\n",
    "            for x in chunks:\n",
    "                values = x.split(': ')\n",
    "                if values[0].strip('\"') == 'journalNameIso':\n",
    "                    journal_names.append(values[1].strip('\"'))\n",
    "                if values[0].strip('\"') == 'pmid':\n",
    "                    pmids.append(values[1].strip('\"'))\n",
    "            wanted_pmids = []\n",
    "            for x in range(len(journal_names)):\n",
    "                if journal_names[x] in ortho_abrvs:\n",
    "                    wanted_pmids.append(pmids[x])\n",
    "    return wanted_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each author query them on ICare. Then cross reference all publications returned with ortho journals. \n",
    "# For those in ortho journals run another ICare query with those PMIDs and store the citation values.\n",
    "\n",
    "data_points = ['total-pubs', 'pubs-per-year', 'cites-per-year-max', 'cites-per-year-mean', 'cites-per-year-sem', 'cites-per-year-med', 'rcr-max', 'rcr-mean', 'rcr-sem', 'rcr-med', 'wrcr']\n",
    "author_name_to_values = {}\n",
    "\n",
    "url = 'https://icite.od.nih.gov/analysis'\n",
    "driver = parse2(url)\n",
    "\n",
    "for x in tqdm(name_dic):\n",
    "    if x not in author_name_to_values:\n",
    "        driver.find_element('id', 'pmid_query').send_keys(x)\n",
    "        driver.find_element(\"id\", 'process-btn').click()\n",
    "        sleep(1)\n",
    "        sourceCode = driver.page_source\n",
    "        soup = BeautifulSoup(sourceCode)\n",
    "\n",
    "        wanted_pmids = find_pmids(soup, ortho_abrvs)\n",
    "\n",
    "        if len(wanted_pmids) > 0:\n",
    "            driver.find_element('xpath', '//a[@href=\"'+'https://icite.od.nih.gov/analysis'+'\"]').click()\n",
    "            pmids_string = \", \".join(wanted_pmids)\n",
    "            driver.find_element('id', 'pmid_text').send_keys(pmids_string)\n",
    "            driver.find_element(\"id\", 'process-btn').click()\n",
    "            sleep(1)\n",
    "            sourceCode = driver.page_source\n",
    "            soup = BeautifulSoup(sourceCode)\n",
    "            author_name_to_values[x] = {data: soup.find_all('td', {'class':data})[0].text for data in data_points}\n",
    "            driver.find_element('xpath', '//a[@href=\"'+'https://icite.od.nih.gov/analysis'+'\"]').click()\n",
    "        else:\n",
    "            driver = parse2(url)\n",
    "            author_name_to_values[x] = {data: 'N/A' for data in data_points}\n",
    "        pickle.dump(author_name_to_values, open('author_name_to_values_5.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
